[TOC]



## 面对突如其来的 GC 问题如何下手解决

最近几年 Java 的版本更新速度是很快的，JVM 的参数配置其实变化也很大。就拿 GC 日志这一块来说，Java 9 几乎是推翻重来。如果你碰到不能被识别的参数，先确认一下自己的 Java 版本。

在事故出现的时候，通常并不是那么温柔。你可能在半夜里就能接到报警电话，这是因为很多定时任务都设定在夜深人静的时候执行。

这个时候，再去看 jstat 已经来不及了，我们需要保留现场。这个便是看门狗的工作，看门狗可以通过设置一些 JVM 参数进行配置。

#### Java 8 的JVM 参数配置：

```java
#!/bin/sh
LOG_DIR="/tmp/logs"
JAVA_OPT_LOG=" -verbose:gc"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -XX:+PrintGCDetails"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -XX:+PrintGCDateStamps"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -XX:+PrintGCApplicationStoppedTime"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -XX:+PrintTenuringDistribution"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -Xloggc:${LOG_DIR}/gc_%p.log"
JAVA_OPT_OOM=" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${LOG_DIR} -XX:ErrorFile=${LOG_DIR}/hs_error_pid%p.log "
JAVA_OPT="${JAVA_OPT_LOG} ${JAVA_OPT_OOM}"
JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow"
```

合成一行。

```java
-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps 
-XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution 
-Xloggc:/tmp/logs/gc_%p.log -XX:+HeapDumpOnOutOfMemoryError 
-XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log 
-XX:-OmitStackTraceInFastThrow
```

然后我们来解释一下这些参数：

| 参数                          | 意义                                                       |
| ----------------------------- | ---------------------------------------------------------- |
| -verbose:gc                   | 打印 GC 日志                                               |
| PrintGCDetails                | 打印详细 GC 日志                                           |
| PrintGCDateStamps             | 系统时间，更加可读，PrintGCTimeStamps 是 JVM 启动时间      |
| PrintGCApplicationStoppedTime | 打印 STW 时间                                              |
| PrintTenuringDistribution     | 打印对象年龄分布，对调优 MaxTenuringThreshold 参数帮助很大 |
| loggc                         | 将以上 GC 内容输出到文件中                                 |

再来看下 OOM 时的参数：

| 参数                       | 意义                       |
| -------------------------- | -------------------------- |
| HeapDumpOnOutOfMemoryError | OOM 时 Dump 信息，非常有用 |
| HeapDumpPath               | Dump 文件保存路径          |
| ErrorFile                  | 错误日志存放路径           |

注意到我们还设置了一个参数 OmitSntackTraceInFastThrow，这是 JVM 用来缩简日志输出的。

开启这个参数之后，如果你多次发生了空指针异常，将会打印以下信息。

```java
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
```

在实际生产中，这个参数是默认开启的，这样就导致有时候排查问题非常不方便（很多研发对此无能为力），我们这里把它关闭，但这样它会输出所有的异常堆栈，日志会多很多。

#### Java 13的JVM 参数配置：

再看下 JDK 13 中的使用。

从 Java 9 开始，移除了 40 多个 GC 日志相关的参数。具体参见 JEP 158。所以这部分的日志配置有很大的变化。

我们同样看一下它的生成脚本。

```java
#!/bin/sh
LOG_DIR="/tmp/logs"
JAVA_OPT_LOG=" -verbose:gc"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file=${LOG_DIR}/gc_%p.log:tags,uptime,time,level"
JAVA_OPT_LOG="${JAVA_OPT_LOG} -Xlog:safepoint:file=${LOG_DIR}/safepoint_%p.log:tags,uptime,time,level"
JAVA_OPT_OOM=" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${LOG_DIR} -XX:ErrorFile=${LOG_DIR}/hs_error_pid%p.log "
JAVA_OPT="${JAVA_OPT_LOG} ${JAVA_OPT_OOM}"
JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow"
echo $JAVA_OPT
```

合成一行展示。

```java
-verbose:gc -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file
=/tmp/logs/gc_%p.log:tags,uptime,time,level -Xlog:safepoint:file=/tmp
/logs/safepoint_%p.log:tags,uptime,time,level -XX:+HeapDumpOnOutOfMemoryError 
-XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log 
-XX:-OmitStackTraceInFastThrow
```

可以看到 GC 日志的打印方式，已经完全不一样，但是比以前的日志参数规整了许多。

#### 什么是safepoint ：

safepoint 是 JVM 中非常重要的一个概念，指的是可以安全地暂停线程的点。

当发生 GC 时，用户线程必须全部停下来，才可以进行垃圾回收，这个状态我们可以认为 JVM 是安全的（safe），整个堆的状态是稳定的。

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/Cgq2xl49MRaAZaFKAABeSE1hLTg491.jpg)

如果在 GC 前，有线程迟迟进入不了 safepoint，那么整个 JVM 都在等待这个阻塞的线程，会造成了整体 GC 的时间变长。

所以呢，并不是只有 GC 会挂起 JVM，进入 safepoint 的过程也会。这个概念，如果你有兴趣可以自行深挖一下，一般是不会出问题的。

如果面试官问起你在项目中都使用了哪些打印 GC 日志的参数，上面这些信息肯定是不很好记忆。你需要进行以下总结。比如：

“我一般在项目中输出详细的 GC 日志，并加上可读性强的 GC 日志的时间戳。特别情况下我还会追加一些反映对象晋升情况和堆详细信息的日志，用来排查问题。另外，OOM 时自动 Dump 堆栈，我一般也会进行配置”。

#### GC 日志的意义：

我们首先看一段日志，然后简要看一下各个阶段的意义。

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/CgpOIF49MRaAUrVfAAFWihJ6jwk874.jpg)

- 1 表示 GC 发生的时间，一般使用可读的方式打印；
- 2 表示日志表明是 G1 的“转移暂停: 混合模式”，停顿了约 223ms；
- 3 表明由 8 个 Worker 线程并行执行，消耗了 214ms；
- 4 表示 Diff 越小越好，说明每个工作线程的速度都很均匀；
- 5 表示外部根区扫描，外部根是堆外区。JNI 引用，JVM 系统目录，Classloaders 等；
- 6 表示更新 RSet 的时间信息；
- 7 表示该任务主要是对 CSet 中存活对象进行转移（复制）；
- 8 表示花在 GC 之外的工作线程的时间；
- 9 表示并行阶段的 GC 总时间；
- 10 表示其他清理活动；
- 11表示收集结果统计；
- 12 表示时间花费统计。

可以看到 GC 日志描述了垃圾回收器过程中的几乎每一个阶段。但即使你了解了这些数值的意义，在分析问题时，也会感到吃力，我们一般使用图形化的分析工具进行分析。

尤其注意的是最后一行日志，需要详细描述。可以看到 G C花费的时间，竟然有 3 个数值。这个数值你可能在多个地方见过。如果你手头有 Linux 机器，可以执行以下命令：

time ls /

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/Cgq2xl49MRaAWpU-AAAUzFIkYlk730.jpg)

可以看到一段命令的执行，同样有三种纬度的时间统计。接下来解释一下这三个字段的意思。

real 实际花费的时间，指的是从开始到结束所花费的时间。比如进程在等待 I/O 完成，这个阻塞时间也会被计算在内；

user 指的是进程在用户态（User Mode）所花费的时间，只统计本进程所使用的时间，注意是指多核；

sys 指的是进程在核心态（Kernel Mode）花费的 CPU 时间量，指的是内核中的系统调用所花费的时间，只统计本进程所使用的时间。

在上面的 GC 日志中，real < user + sys，因为我们使用了多核进行垃圾收集，所以实际发生的时间比 (user + sys) 少很多。在多核机器上，这很常见。

[Times: user=1.64 sys=0.00, real=0.23 secs]



下面是一个串行垃圾收集器收集的 GC 时间的示例。由于串行垃圾收集器始终仅使用一个线程，因此实际使用的时间等于用户和系统时间的总和：

[Times: user=0.29 sys=0.00, real=0.29 secs]

那我们统计 GC 以哪个时间为准呢？一般来说，用户只关心系统停顿了多少秒，对实际的影响时间非常感兴趣。至于背后是怎么实现的，是多核还是单核，是用户态还是内核态，它们都不关心。所以我们直接使用 real 字段。

#### GC日志可视化：

使用gceasy 查看日志

以下是一个使用了 G1 垃圾回收器，堆内存为 6GB 的服务，运行 5 天的 GC 日志。

（1）堆信息

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/CgpOIF49MRaACSuzAABDthdVxTk570.jpg)

我们可以从图中看到堆的使用情况。

（2）关键信息

从图中我们可以看到一些性能的关键信息。

吞吐量：98.6%（一般超过 95% 就 ok 了）；

最大延迟：230ms，平均延迟：42.8ms；

延迟要看服务的接受程度，比如 SLA 定义 50ms 返回数据，上面的最大延迟就会有一点问题。本服务接近 99% 的停顿在 100ms 以下，可以说算是非常优秀了。

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/Cgq2xl49MReAI98WAABRPqHhDjE672.jpg)

你在看这些信息的时候，一定要结合宿主服务器的监控去看。比如 GC 发生期间，CPU 会突然出现尖锋，就证明 GC 对 CPU 资源使用的有点多。但多数情况下，如果吞吐量和延迟在可接受的范围内，这些对 CPU 的超额使用是可以忍受的。

（3）交互式图表

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/CgpOIF49MReAcKfGAABakc1dRtA053.jpg)

可以对有问题的区域进行放大查看，图中表示垃圾回收后的空间释放，可以看到效果是比较好的。

（4）G1 的时间耗时

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/Cgq2xl49MReAf_DdAACM8OnUC_I541.jpg)

如图展示了 GC 的每个阶段花费的时间。可以看到平均耗时最长的阶段，就是 Concurrent Mark 阶段，但由于是并发的，影响并不大。随着时间的推移，YoungGC 竟然达到了 136485 次。运行 5 天，光花在 GC 上的时间就有 2 个多小时，还是比较可观的。

（5）其他

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/CgpOIF49MReAL2goAAB6BiE3imA217.jpg)

如图所示，整个 JVM 创建了 100 多 T 的数据，其中有 2.4TB 被 promoted 到老年代。

另外，还有一些 safepoint 的信息等，你可以自行探索。

那到底什么样的数据才是有问题的呢？gceasy 提供了几个案例。比如下面这个就是停顿时间明显超长的 GC 问题。

![img](https://s0.lgstatic.com/i/image3/M01/64/AC/Cgq2xl49MReAQIQ_AABZPnGfj9s030.jpg)

下面这个是典型的内存泄漏。

![](D:\study\学习资料\笔记\JVM\图片\第九讲\11.jpg)

上面这些问题都是非常明显的。但大多数情况下，问题是偶发的。从基本的衡量指标，就能考量到整体的服务水准。如果这些都没有问题，就要看曲线的尖峰。

一般来说，任何不平滑的曲线，都是值得怀疑的，那就需要看一下当时的业务情况具体是什么样子的。是用户请求突增引起的，还是执行了一个批量的定时任务，再或者查询了大批量的数据，这要和一些服务的监控一起看才能定位出根本问题。

只靠 GC 来定位问题是比较困难的，我们只需要知道它有问题就可以了。后面，会介绍更多的支持工具进行问题的排解。

为了方便你调试使用，我在 GitHub 上上传了两个 GC 日志。其中 gc01.tar.gz 就是我们现在正在看的，解压后有 200 多兆；另外一个 gc02.tar.gz 是一个堆空间为 1GB 的日志文件，你也可以下载下来体验一下。

GitHub 地址：

https://gitee.com/xjjdog/jvm-lagou-res

#### jstat：

上面的可视化工具，必须经历导出、上传、分析三个阶段，这种速度太慢了。有没有可以实时看堆内存的工具？

你可能会第一时间想到 jstat 命令。第一次接触这个命令，我也是很迷惑的，主要是输出的字段太多，不了解什么意义。

但其实了解我们在前几节课时所讲到内存区域划分和堆划分之后，再看这些名词就非常简单了。

![](D:\study\学习资料\笔记\JVM\图片\第九讲\12.jpg)

我们拿 -gcutil 参数来说明一下。

jstat -gcutil $pid 1000

只需要提供一个 Java 进程的 ID，然后指定间隔时间（毫秒）就 OK 了。

```java
S0 S1 E O M CCS YGC YGCT FGC FGCT GCT
0.00 0.00 72.03 0.35 54.12 55.72 11122 16.019 0 0.000 16.019
0.00 0.00 95.39 0.35 54.12 55.72 11123 16.024 0 0.000 16.024
0.00 0.00 25.32 0.35 54.12 55.72 11125 16.025 0 0.000 16.025
0.00 0.00 37.00 0.35 54.12 55.72 11126 16.028 0 0.000 16.028
0.00 0.00 60.35 0.35 54.12 55.72 11127 16.028 0 0.000 16.028
```


可以看到，E 其实是 Eden 的缩写，S0 对应的是 Surivor0，S1 对应的是 Surivor1，O 代表的是 Old，而 M 代表的是 Metaspace。

YGC 代表的是年轻代的回收次数，YGC T对应的是年轻代的回收耗时。那么 FGC 肯定代表的是 Full GC 的次数。

你在看日志的时候，一定要注意其中的规律。-gcutil 位置的参数可以有很多种。我们最常用的有 gc、gcutil、gccause、gcnew 等，其他的了解一下即可。

- gc: 显示和 GC 相关的堆信息；

- gcutil: 显示垃圾回收信息；

- gccause: 显示垃圾回收的相关信息（同 -gcutil），同时显示最后一次或当前正在发生的垃圾回收的诱因；

- gcnew: 显示新生代信息；

- gccapacity: 显示各个代的容量以及使用情况；

- gcmetacapacity: 显示元空间 metaspace 的大小；

- gcnewcapacity: 显示新生代大小和使用情况；

- gcold: 显示老年代和永久代的信息；

- gcoldcapacity: 显示老年代的大小；

- printcompilation: 输出 JIT 编译的方法信息；

- class: 显示类加载 ClassLoader 的相关信息；

- compiler: 显示 JIT 编译的相关信息；


如果 GC 问题特别明显，通过 jstat 可以快速发现。我们在启动命令行中加上参数 -t，可以输出从程序启动到现在的时间。如果 FGC 和启动时间的比值太大，就证明系统的吞吐量比较小，GC 花费的时间太多了。另外，如果老年代在 Full GC 之后，没**有明显的下降，那可能内存已经达到了瓶颈**，**或者有内存泄漏问题**。

下面这行命令，就追加了 GC 时间的增量和 GC 时间比率两列。

```java
jstat -gcutil -t 90542 1000 | awk 'BEGIN{pre=0}{if(NR>1) {print $0 "\t" ($12-pre) "\t" $12*100/$1 ; pre=$12 } else { print $0 "\tGCT_INC\tRate"} }'

Timestamp         S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT    GCT_INC Rate
           18.7   0.00 100.00   6.02   1.45  84.81  76.09      1    0.002     0    0.000    0.002 0.002 0.0106952
           19.7   0.00 100.00   6.02   1.45  84.81  76.09      1    0.002     0    0.000    0.002 0 0.0101523
```

#### GC 日志也会搞鬼（这部分等学了 ElasticSearch再来看）

顺便给你介绍一个实际发生的故障。

你知道 ElasticSearch 的速度是非常快的，我们为了压榨它的性能，对磁盘的读写几乎是全速的。它在后台做了很多 Merge 动作，将小块的索引合并成大块的索引。还有 TransLog 等预写动作，都是 I/O 大户。

使用 iostat -x 1 可以看到具体的 I/O 使用状况。（ iostat -x 1 ：查看磁盘的IO负载）

```java
//Linux系统出现了性能问题，一般我们可以通过top.iostat,vmstat等命令来查看初步定位问题。其中iostat可以给我们提供丰富的IO状态数据
$ iostat -x -1 
avg-cpu:  %user   %nice %system %iowait  %steal   %idle

          10.43    0.00    1.51    1.51    0.00   86.56

Device:rrqm/s  wrqm/s  r/s   w/s  rkB/s  wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util

sda 4477.00   9.00  951.00  13.00 24288.00 2492.00 55.56   0.21    0.22    0.21    0.92   0.17  16.00

%user :Show the percentage  of  CPU  utilization  that  occurred while executing at the user level (application).

%nice  :Show  the  percentage  of  CPU  utilization that occurred while executing at the user level with nice priority.                

%system: Show the percentage  of  CPU  utilization  that  occurred  while executing at the system level (kernel).
    
%iowait:Show  the  percentage  of  time that the CPU or CPUs were  idle during which the system had an  outstanding disk  I/O  request.     
    
%steal :Show  the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hypervisor was  servicing another virtual processor.                  
%idle :Show  the  percentage  of  time that the CPU or CPUs were idle and the system did not have an outstanding disk  I/O   request.   
    
//rrqm/s:每秒进行merge的读操作数目。即delta(rmerge)/s 
//wrqm/s:每秒进行merge的写操作数目。即delta(wmerge)/s 
//r/s:每秒完成的读I/O设备次数。即delta(rio)/s 
//w/s:每秒完成的写I/0设备次数。即delta(wio)/s 
//rsec/s:每秒读扇区数。即delta(rsect)/s 
//wsec/s:每秒写扇区数。即delta(wsect)/s 
//rKB/s:每秒读K字节数。是rsec/s的一半，因为每扇区大小为512字节 
//wKB/s:每秒写K字节数。是wsec/s的一半 
//avgrq-sz:平均每次设备I/O操作的数据大小(扇区)。即delta(rsect+wsect)/delta(rio+wio) 
//avgqu-sz:平均I/O队列长度。即delta(aveq)/s/1000(因为aveq的单位为毫秒) 
//await:平均每次设备I/O操作的等待时间(毫秒)。即delta(ruse+wuse)/delta(rio+wio) 
//svctm:平均每次设备I/O操作的服务时间(毫秒)。即delta(use)/delta(rio+wio) 
//%util:一秒中有百分之多少的时间用于I/O操作,或者说一秒中有多少时间I/O队列是非空的。即delta(usr)/s/1000(因为use的单位为毫秒) 

//如果%util接近100%,表明I/O请求太多,I/O系统已经满负荷，磁盘可能存在瓶颈,一般%util大于70%,I/O压力就比较大.
//svctm一般要小于await(因为同时等待的请求的等待时间被重复计算了),svctm的大小一般和磁盘性能有关,CPU/内存的负荷也会对其有影响，请求过多也会 间接导致svctm的增加。await的大小一般取决于服务时间(svctm)以及I/O队列的长度和I/O请求的发出模式。如果svctm比较接近await,说明I/O几乎没有 等待时间;如果await远大于svctm,说明I/O队列太长，应用得到的响应时间变慢,如果响应时间超过了用户可以容许的范围,这时可以考虑更换更快的磁盘,调 整内核elevator算法,优化应用,或者升级CPU 
    
//队列长度(avcqu-sz)也可作为衡量系统I/O负荷的指标,但由于avcqu-sz是按照单位时间的平均值,所以不能反映瞬间的I/O洪水。    
```

问题是，我们有一套 ES 集群，在访问高峰时，有多个 ES 节点发生了严重的 STW 问题。有的节点竟停顿了足足有 7~8 秒。

 [Times: user=0.42 sys=0.03, real=7.62 secs] 

从日志可以看到在 GC 时用户态只停顿了 420ms，但真实的停顿时间却有 7.62 秒。

盘点一下资源，唯一超额利用的可能就是 I/O 资源了（%util 保持在 90 以上），GC 可能在等待 I/O。

通过搜索，发现已经有人出现过这个问题，这里直接说原因和结果。

原因就在于，写 GC 日志的 write 动作，是统计在 STW 的时间里的。在我们的场景中，由于 ES 的索引数据，和 GC 日志放在了一个磁盘，GC 时写日志的动作，就和写数据文件的动作产生了资源争用。

![](D:\study\学习资料\笔记\JVM\图片\第九讲\13.jpg)

解决方式也是比较容易的，把 ES 的日志文件，单独放在一块普通 HDD 磁盘上就可以了。



## 模拟 JVM 内存溢出场景

在开始之前，请你下载并安装一个叫作 VisualVM 的工具，我们使用这个图形化的工具看一下溢出过程。

### 14个JVM内存配置参数：

#### 1.栈内存大小相关设置

##### -Xss1024k

- 意义： 设置线程栈占用内存大小。
- 默认值：不同的操作系统平台，其默认值不同，具体看官网说明。

#### 2.堆内存大小相关设置

##### -Xms512m

- 意义： 设置堆内存初始值大小。
- 默认值：如果未设置，初始值将是老年代和年轻代分配制内存之和。

##### -Xmx1024m

- 意义： 设置堆内存最大值。
- 默认值：default value is chosen at runtime based on system configuration,具体请查看官网或者查看讨论[How is the default Java heap size determined?](https://stackoverflow.com/questions/4667483/how-is-the-default-java-heap-size-determined)。

#### 3.年轻代内存大小相关设置

##### -Xmn512m

- 意义： 设置新生代的初始值及最大值。
- 默认值：堆内存的1/4（这里要记住不是最大堆内存，还是已经分配的堆内存的1/4）。

##### -XX:NewSize=512m

- 意义：设置新生代的初始值。

##### -XX:MaxNewSize=512m

- 意义：设置新生代的最大值。

#### 4.比率方式设置

##### -XX:NewRatio=8

- 意义：设置老年代和年轻代的比例。比如：-XX:NewRatio=8 表示`老年代内存:年轻代内存=8:1 => 老年代占堆内存的8/9;年轻代占堆内存的1/9`。
- 默认值：2 。

##### -XX:SurvivorRatio=32

- 意义：设置新生代和存活区的比例（这里需要注意的是存活区指的是其中一个）。比如：-XX:SurvivorRatio=8 表示`存活区：新生代=1：8 =》新生代占年轻代的8/10,每个存活区各占年轻代的1/10`。
- 默认值：8 。

##### -XX:MinHeapFreeRatio=40

- 意义：GC后，如果发现空闲堆内存占到整个预估上限值的40%，则增大上限值。
- 默认值：40 。

##### -XX:MaxHeapFreeRatio=70

- 意义：GC后，如果发现空闲堆内存占到整个预估上限值的70%，则收缩预估上限值。
- 默认值：70。

#### 5.Meta大小相关设置

##### -XX:MetaspaceSize=128m

- 意义：初始元空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。
- 默认值：依赖平台。

##### -XX:MaxMetaspaceSize=256m

- 意义：设置元空间的最大值，默认是没有上限的，也就是说你的系统内存上限是多少它就是多少。
- 默认值：默认没有上限，在技术上，Metaspace的尺寸可以增长到交换空间。

>  以上就是14个参数，为了深刻理解，建议本地配置让后观察内存大小变化（可以使用jmap -heap pid 或者 visualGC来帮助观察）验证自己的理解是否正确。



### 模拟堆溢出：

```java
public class TestOOM {
    public static class OOMobject{
        private byte[] bytes=new byte[1024*1024];
    }
    public static void main(String[] args) {
        List<OOMobject> list=new ArrayList<>();
        while (true){
            list.add(new OOMobject());
        }
    }
}
```

我们使用 CMS 收集器进行垃圾回收，可以看到如下的信息。

#### 命令：

java -Xmx20m  -Xmn4m   -XX:+UseConcMarkSweepGC  -verbose:gc -Xlog:gc,

gc+ref=debug,gc+heap=debug,

gc+age=trace:file=/tmp/logs/gc_%p.log:tags,

uptime,

time,

level -Xlog:safepoint:file=/tmp/logs/safepoint_%p.log:tags,

uptime,

time,

level -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow  OOMTest

#### 输出：

[0.025s][info][gc] Using Concurrent Mark Sweep

0MB

CodeHeap 'non-nmethods'  committed:2555904  used:1120512

Metaspace  committed:4980736  used:854432

CodeHeap 'profiled nmethods'  committed:2555904  used:265728

Compressed Class Space  committed:524288  used:96184

Par Eden Space  committed:3407872  used:2490984

Par Survivor Space  committed:393216  used:0

CodeHeap 'non-profiled nmethods'  committed:2555904  used:78592

CMS Old Gen  committed:16777216  used:0

...省略

[16.377s][info][gc] GC(9) Concurrent Mark 1.592ms

[16.377s][info][gc] GC(9) Concurrent Preclean

[16.378s][info][gc] GC(9) Concurrent Preclean 0.721ms

[16.378s][info][gc] GC(9) Concurrent Abortable Preclean

[16.378s][info][gc] GC(9) Concurrent Abortable Preclean 0.006ms

[16.378s][info][gc] GC(9) Pause Remark 17M->17M(19M) 0.344ms

[16.378s][info][gc] GC(9) Concurrent Sweep

[16.378s][info][gc] GC(9) Concurrent Sweep 0.248ms

[16.378s][info][gc] GC(9) Concurrent Reset

[16.378s][info][gc] GC(9) Concurrent Reset 0.013ms

17MB

CodeHeap 'non-nmethods'  committed:2555904  used:1120512

Metaspace  committed:4980736  used:883760

CodeHeap 'profiled nmethods'  committed:2555904  used:422016

Compressed Class Space  committed:524288  used:92432

Par Eden Space  committed:3407872  used:3213392

Par Survivor Space  committed:393216  used:0

CodeHeap 'non-profiled nmethods'  committed:2555904  used:88064

CMS Old Gen  committed:16777216  used:16452312

[18.380s][info][gc] GC(10) Pause Initial Mark 18M->18M(19M) 0.187ms

[18.380s][info][gc] GC(10) Concurrent Mark

[18.384s][info][gc] GC(11) Pause Young (Allocation Failure) 18M->18M(19M) 0.186ms

[18.386s][info][gc] GC(10) Concurrent Mark 5.435ms

[18.395s][info][gc] GC(12) Pause Full (Allocation Failure) 18M->18M(19M) 10.572ms

[18.400s][info][gc] GC(13) Pause Full (Allocation Failure) 18M->18M(19M) 5.348ms

**Exception in thread "main" java.lang.OutOfMemoryError: Java heap space**

   **at OldOOM.main(OldOOM.java:20)**

VisualVM 的截图展示了这个溢出结果。可以看到 Eden 区刚开始还是运行平稳的，内存泄漏之后就开始疯狂回收（其实是提升），老年代内存一直增长，直到 OOM。

####  VisualVM图：

![img](https://s0.lgstatic.com/i/image3/M01/65/E6/Cgq2xl5DytGAc09TAAFdBh0n9eo313.jpg)

### 模拟元空间溢出：

堆一般都是指定大小的，但元空间不是。所以如果元空间发生内存溢出会更加严重，会造成操作系统的内存溢出。我们在使用的时候，也会给它设置一个上限 for safe。

元空间溢出主要是由于加载的类太多，或者动态生成的类太多。

```java
public class MetaspaceOOMTest {
   public interface Facade {
       void m(String input);
   }
   public static class FacadeImpl implements Facade {
       @Override
       public void m(String name) {
       }
   }
   public static class MetaspaceFacadeInvocationHandler implements InvocationHandler {
       private Object impl;
       public MetaspaceFacadeInvocationHandler(Object impl) {
           this.impl = impl;
       }
       @Override
       public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
           return method.invoke(impl, args);
       }
   }
   private static Map<String, Facade> classLeakingMap = new HashMap<String, Facade>();
   private static void oom(HttpExchange exchange) {
       try {
           String response = "oom begin!";
           exchange.sendResponseHeaders(200, response.getBytes().length);
           OutputStream os = exchange.getResponseBody();
           os.write(response.getBytes());
           os.close();
       } catch (Exception ex) {
       }
       try {
           for (int i = 0; ; i++) {
               String jar = "file:" + i + ".jar";
               URL[] urls = new URL[]{new URL(jar)};
               URLClassLoader newClassLoader = new URLClassLoader(urls);
               Facade t = (Facade) Proxy.newProxyInstance(newClassLoader,
                       new Class<?>[]{Facade.class},
                       new MetaspaceFacadeInvocationHandler(new FacadeImpl()));
               classLeakingMap.put(jar, t);
           }
       } catch (Exception e) {
       }
   }
   private static void srv() throws Exception {
       HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0);
       HttpContext context = server.createContext("/");
       context.setHandler(MetaspaceOOMTest::oom);
       server.start();
   }
   public static void main(String[] args) throws Exception {
       srv();
   }
}
```

这段代码将使用 Java 自带的动态代理类，不断的生成新的 class。

java -Xmx20m  -Xmn4m   -XX:+UseG1GC  -verbose:gc -Xlog:gc,gc+ref=debug,gc+heap=debug,gc+age=trace:file=/tmp/logs/gc_%p.log:tags,uptime,time,level -Xlog:safepoint:file=/tmp/logs/safepoint_%p.log:tags,uptime,time,level -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs -XX:ErrorFile=/tmp/logs/hs_error_pid%p.log -XX:-OmitStackTraceInFastThrow -XX:MetaspaceSize=16M -XX:MaxMetaspaceSize=16M  MetaspaceOOMTest

我们在启动的时候，限制 Metaspace 空间大小为 16MB。可以看到运行一小会之后，Metaspace 会发生内存溢出。

[6.509s][info][gc] GC(28) Pause Young (Concurrent Start) (Metadata GC Threshold) 9M->9M(20M) 1.186ms

[6.509s][info][gc] GC(30) Concurrent Cycle

[6.534s][info][gc] GC(29) Pause Full (Metadata GC Threshold) 9M->9M(20M) 25.165ms

[6.556s][info][gc] GC(31) Pause Full (Metadata GC Clear Soft References) 9M->9M(20M) 21.136ms

[6.556s][info][gc] GC(30) Concurrent Cycle 46.668ms

**java.lang.OutOfMemoryError: Metaspace**

Dumping heap to /tmp/logs/java_pid36723.hprof ...

Heap dump file created [17362313 bytes in 0.134 secs]

####  VisualVM图：

![img](https://s0.lgstatic.com/i/image3/M01/65/E6/Cgq2xl5DytKAMGfeAAE5q9rh1rM558.jpg)

但假如你把堆 Metaspace 的限制给去掉，会更可怕。它占用的内存会一直增长。

### 模拟堆外内存溢出：

严格来说，上面的 Metaspace 也是属于堆外内存的。但是我们这里的堆外内存指的是 Java 应用程序通过直接方式从操作系统中申请的内存。所以严格来说，这里是指直接内存。

程序将通过 ByteBuffer 的 **allocateDirect 方法**每 1 秒钟申请 1MB 的直接内存。

```java
public class OffHeapOOMTest {
   public static final int _1MB = 1024 * 1024;
   static List<ByteBuffer> byteList = new ArrayList<>();
   private static void oom(HttpExchange exchange) {
       try {
           String response = "oom begin!";
           exchange.sendResponseHeaders(200, response.getBytes().length);
           OutputStream os = exchange.getResponseBody();
           os.write(response.getBytes());
           os.close();
       } catch (Exception ex) {
       }
       for (int i = 0; ; i++) {
           ByteBuffer buffer = ByteBuffer.allocateDirect(_1MB);
           byteList.add(buffer);
           System.out.println(i + "MB");
           memPrint();
           try {
               Thread.sleep(1000);
           } catch (Exception e) {
           }
       }
   }
   private static void srv() throws Exception {
       HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0);
       HttpContext context = server.createContext("/");
       context.setHandler(OffHeapOOMTest::oom);
       server.start();
   }
   public static void main(String[] args) throws Exception {
       srv();
   }
   static void memPrint() {
       for (MemoryPoolMXBean memoryPoolMXBean : ManagementFactory.getMemoryPoolMXBeans()) {
           System.out.println(memoryPoolMXBean.getName() +
                   "  committed:" + memoryPoolMXBean.getUsage().getCommitted() +
                   "  used:" + memoryPoolMXBean.getUsage().getUsed());
       }
   }
}
```

以下是程序运行一段时间抛出的错误。

Exception in thread "Thread-2" **java.lang.OutOfMemoryError: Direct buffer memory**

   at java.nio.Bits.reserveMemory(Bits.java:694)

   at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)

   at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)

   at OffHeapOOMTest.oom(OffHeapOOMTest.java:27)

   at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)

   at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)

   at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)

   at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)

   at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)

   at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)

   at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:158)

   at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:431)

   at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:396)

   at java.lang.Thread.run(Thread.java:748)

### 模拟栈溢出：

栈溢出指的就是这里的数据太多造成的泄漏。通过 -Xss 参数可以设置它的大小。

从这里我们也能了解到，由于每个线程都有一个虚拟机栈。线程的开销也是要占用内存的。如果系统中的线程数量过多，那么占用内存的大小也是非常可观的。

栈溢出不会造成 JVM 进程死亡，危害“相对较小”。下面是一个简单的模拟栈溢出的代码，只需要递归调用就可以了。

```java
public class StackOverflowTest {
   static int count = 0;
   static void a() {
       System.out.println(count);
       count++;
       b();
   }
   static void b() {
       System.out.println(count);
       count++;
       a();
   }
   public static void main(String[] args) throws Exception {
       a();
   }
}
```

运行后，程序直接报错。

Exception in thread "main" java.lang.StackOverflowError

   at java.io.PrintStream.write(PrintStream.java:526)

   at java.io.PrintStream.print(PrintStream.java:597)

   at java.io.PrintStream.println(PrintStream.java:736)

   at StackOverflowTest.a(StackOverflowTest.java:5)

如果你的应用经常发生这种情况，可以试着调大这个值。但一般都是因为程序错误引起的，最好检查一下自己的代码。

### 进程异常退出怎么办

##### 进程异常退出的场景：

随着使用内存越用越多。第一层防护墙就是 SWAP；当 SWAP 也用的差不多了，会尝试释放 cache；当这两者资源都耗尽，杀手就出现了。oom-killer 会在系统内存耗尽的情况下跳出来，选择性的干掉一些进程以求释放一点内存。

比如一共 8GB 的机器，你把整整 7.5GB 都分配给了 JVM。当操作系统内存不足时，你的 JVM 就可能被操作系统杀掉了。

##### 解决办法：

1. 使用dmesg -T（加-T是看到发生时间）查看进程崩溃信息。

   dmesg 作用：

   ![image-20210201202939139](C:\Users\12031\AppData\Roaming\Typora\typora-user-images\image-20210201202939139.png)

2. 禁用System.exit() 函数，这个函数相对于被动终结，还有一种主动求死的方式。

3. 在启动命令前面加nohup和&，例：

   ```java
   nohup command & //command为某个启动命令
   ```

   nohup：全称（ no hang up），如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。

   &：以期望进程在后台运行。

4. 关闭服务的时候，使用“kill -15”，而不是“kill -9”。

   ##### Kill -9 和Kill -15 有什么区别？

   - kill -9 PID 是操作系统从内核级别强制杀死一个进程.

   - kill -15 PID 可以理解为操作系统发送一个通知告诉应用主动关闭.

     kill -15 执行后，可能会发生以下几种情况的一种：

     1. 当前程序立刻停止；
     2. 程序释放相应资源，然后再停止；
     3. 程序可能仍然继续运行。

### GC 引起 CPU 飙升怎么排查：

##### 在 Linux 上，分析哪个线程引起的 CPU 问题，通常有一个固定的步骤，这是面试频率极高的一个问题。：

![img](https://s0.lgstatic.com/i/image3/M01/66/80/CgpOIF5GcZ-AcGzzAAAmNdRr-Xo623.jpg)

（1）使用 top 命令，查找到使用 CPU 最多的某个进程，记录它的 pid。使用 Shift + P 快捷键可以按 CPU 的使用率进行排序。

```java
top
```


（2）再次使用 top 命令，加 -H 参数，查看某个进程中使用 CPU 最多的某个线程，记录线程的 ID。

```javascript
top -Hp $pid
```


（3）使用 printf 函数，将十进制的 tid 转化成十六进制。

```java
printf %x $tid
```


（4）使用 jstack 命令，查看 Java 进程的线程栈。

```javascript
jstack $pid >$pid.log
```


（5）使用 less 命令查看生成的文件，并查找刚才转化的十六进制 tid，找到发生问题的线程上下文。

```java
less $pid.log
```


我们在 jstack 日志中找到了 CPU 使用最多的几个线程。

![img](https://s0.lgstatic.com/i/image3/M01/66/80/Cgq2xl5GcZ-AYE1iAAD_AB7LDNA381.jpg)

可以看到问题发生的根源，是我们的堆已经满了，但是又没有发生 OOM，于是 GC 进程就一直在那里回收，回收的效果又非常一般，造成 CPU 升高应用假死。

接下来的具体问题排查，就需要把内存 dump 一份下来，使用 MAT 等工具分析具体原因了（将在第 12 课时讲解）。

### 瞬时态和历史态：

- 瞬时态是指当时发生的、快照类型的元素；

- 历史态是指**按照频率抓取**的，有固定监控项的资源变动图。

有很多信息，比如 **CPU、系统内存**等，瞬时态的价值就不如历史态来的直观一些。因为瞬时状态无法体现一个趋势性问题（比如斜率、求导等），而这些信息的获取一般依靠监控系统的协作。

但对于 **lsof、heap** 等，这种没有时间序列概念的混杂信息，体积都比较大，无法进入监控系统产生有用价值，**就只能通过瞬时态进行分析。**在这种情况下，瞬时态的价值反而更大一些。我们常见的堆快照，就属于瞬时状态。

问题不是凭空产生的，在分析时，一般要收集系统的整体变更集合，比如代码变更、网络变更，甚至数据量的变化。

![img](https://s0.lgstatic.com/i/image3/M01/66/80/CgpOIF5GcZ-AM6HBAACH1_ojfyo889.jpg)

### 保留信息命令：

（1）系统当前网络连接

```java
ss -antp > $DUMP_DIR/ss.dump 2>&1
```


其中，ss 命令将系统的所有网络连接输出到 ss.dump 文件中。使用 ss 命令而不是 netstat 的原因，是因为 netstat 在网络连接非常多的情况下，执行非常缓慢。

后续的处理，可通过查看各种网络连接状态的梳理，来排查 TIME_WAIT 或者 CLOSE_WAIT，或者其他连接过高的问题，非常有用。

线上有个系统更新之后，监控到 CLOSE_WAIT 的状态突增，最后整个 JVM 都无法响应。CLOSE_WAIT 状态的产生一般都是代码问题，使用 jstack 最终定位到是因为 HttpClient 的不当使用而引起的，多个连接不完全主动关闭。

（2）网络状态统计

```java
netstat -s > $DUMP_DIR/netstat-s.dump 2>&1
```


此命令将网络统计状态输出到 netstat-s.dump 文件中。它能够按照各个协议进行统计输出，对把握当时整个网络状态，有非常大的作用。

```java
sar -n DEV 1 2 > $DUMP_DIR/sar-traffic.dump 2>&1
```


上面这个命令，会使用 sar 输出当前的网络流量。在一些速度非常高的模块上，比如 Redis、Kafka，就经常发生跑满网卡的情况。如果你的 Java 程序和它们在一起运行，资源则会被挤占，表现形式就是网络通信非常缓慢。

（3）进程资源

```java
lsof -p $PID > $DUMP_DIR/lsof-$PID.dump
```


这是个非常强大的命令，通过查看进程，能看到打开了哪些文件，这是一个神器，可以以进程的维度来查看整个资源的使用情况，包括每条网络连接、每个打开的文件句柄。同时，也可以很容易的看到连接到了哪些服务器、使用了哪些资源。这个命令在资源非常多的情况下，输出稍慢，请耐心等待。

（4）CPU 资源

```java
mpstat > $DUMP_DIR/mpstat.dump 2>&1
vmstat 1 3 > $DUMP_DIR/vmstat.dump 2>&1
sar -p ALL  > $DUMP_DIR/sar-cpu.dump  2>&1
uptime > $DUMP_DIR/uptime.dump 2>&1
```


主要用于输出当前系统的 CPU 和负载，便于事后排查。这几个命令的功能，有不少重合，使用者要注意甄别。

（5）I/O 资源

```java
iostat -x > $DUMP_DIR/iostat.dump 2>&1
```


一般，以计算为主的服务节点，I/O 资源会比较正常，但有时也会发生问题，比如日志输出过多，或者磁盘问题等。此命令可以输出每块磁盘的基本性能信息，用来排查 I/O 问题。在第 8 课时介绍的 GC 日志分磁盘问题，就可以使用这个命令去发现。

（6）内存问题

```java
free -h > $DUMP_DIR/free.dump 2>&1
```


free 命令能够大体展现操作系统的内存概况，这是故障排查中一个非常重要的点，比如 SWAP 影响了 GC，SLAB 区挤占了 JVM 的内存。

（7）其他全局

```java
ps -ef > $DUMP_DIR/ps.dump 2>&1
dmesg > $DUMP_DIR/dmesg.dump 2>&1
sysctl -a > $DUMP_DIR/sysctl.dump 2>&1
```


dmesg 是许多静悄悄死掉的服务留下的最后一点线索。当然，ps 作为执行频率最高的一个命令，它当时的输出信息，也必然有一些可以参考的价值。

另外，由于内核的配置参数，会对系统和 JVM 产生影响，所以我们也输出了一份。

（8）进程快照，最后的遗言（jinfo）

```java
${JDK_BIN}jinfo $PID > $DUMP_DIR/jinfo.dump 2>&1
```


此命令将输出 Java 的基本进程信息，包括环境变量和参数配置，可以查看是否因为一些错误的配置造成了 JVM 问题。

（9）dump 堆信息

```java
${JDK_BIN}jstat -gcutil $PID > $DUMP_DIR/jstat-gcutil.dump 2>&1
${JDK_BIN}jstat -gccapacity $PID > $DUMP_DIR/jstat-gccapacity.dump 2>&1
```


jstat 将输出当前的 gc 信息。一般，基本能大体看出一个端倪，如果不能，可将借助 jmap 来进行分析。

（10）堆信息

```java
${JDK_BIN}jmap $PID > $DUMP_DIR/jmap.dump 2>&1
${JDK_BIN}jmap -heap $PID > $DUMP_DIR/jmap-heap.dump 2>&1
${JDK_BIN}jmap -histo $PID > $DUMP_DIR/jmap-histo.dump 2>&1
${JDK_BIN}jmap -dump:format=b,file=$DUMP_DIR/heap.bin $PID > /dev/null  2>&1
```


jmap 将会得到当前 Java 进程的 dump 信息。如上所示，其实最有用的就是第 4 个命令，但是前面三个能够让你初步对系统概况进行大体判断。

因为，第 4 个命令产生的文件，一般都非常的大。而且，需要下载下来，导入 MAT 这样的工具进行深入分析，才能获取结果。这是分析内存泄漏一个必经的过程。

（11）JVM 执行栈

```java
${JDK_BIN}jstack $PID > $DUMP_DIR/jstack.dump 2>&1
```


jstack 将会获取当时的执行栈。一般会多次取值，我们这里取一次即可。这些信息非常有用，能够还原 Java 进程中的线程情况。

```java
top -Hp $PID -b -n 1 -c >  $DUMP_DIR/top-$PID.dump 2>&1
```


为了能够得到更加精细的信息，我们使用 top 命令，来获取进程中所有线程的 CPU 信息，这样，就可以看到资源到底耗费在什么地方了。

（12）高级替补

```java
kill -3 $PID
```


有时候，jstack 并不能够运行，有很多原因，比如 Java 进程几乎不响应了等之类的情况。我们会尝试向进程发送 kill -3 信号，这个信号将会打印 jstack 的 trace 信息到日志文件中，是 jstack 的一个替补方案。

```java
gcore -o $DUMP_DIR/core $PID
```


对于 jmap 无法执行的问题，也有替补，那就是 GDB 组件中的 gcore，将会生成一个 core 文件。我们可以使用如下的命令去生成 dump：

```java
${JDK_BIN}jhsdb jmap --exe ${JDK}java  --core $DUMP_DIR/core --binaryheap
```

### 内存泄漏的现象：

稍微提一下 jmap 命令，它在 9 版本里被干掉了，取而代之的是 jhsdb，你可以像下面的命令一样使用。

```java
jhsdb --heap --pid  37340 //或jmap --heap --pid  37340
jhsdb --pid  37288//或jmap  --pid  37288
jhsdb --histo --pid  37340//或jmap  --histo --pid  37340
jhsdb --binaryheap --pid  37340//或jmap  --binaryheap --pid  37340
```

![img](https://img2018.cnblogs.com/blog/978388/201906/978388-20190626140755576-1212446631.png)

heap 参数能够帮我们看到大体的内存布局，以及每一个年代中的内存使用情况。这和我们前面介绍的内存布局，以及在 VisualVM 中看到的 没有什么不同。但由于它是命令行，所以使用更加广泛。

![img](https://s0.lgstatic.com/i/image3/M01/66/80/Cgq2xl5GcZ-ALFLkAADxjrTYlzI318.jpg)

histo 能够大概的看到系统中每一种类型占用的空间大小，用于初步判断问题。比如某个对象 instances 数量很小，但占用的空间很大，这就说明存在大对象。但它也只能看大概的问题，要找到具体原因，还是要 dump 出当前 live 的对象。

![img](https://s0.lgstatic.com/i/image3/M01/66/80/CgpOIF5GcZ-AKpQpAADVkT6rMe0124.jpg)

一般内存溢出，表现形式就是 Old 区的占用持续上升，即使经过了多轮 GC 也没有明显改善。我们在前面提到了 GC Roots，内存泄漏的根本就是，有些对象并没有切断和 GC Roots 的关系，可通过一些工具，能够看到它们的联系。

### Linux swap分区详解：

swap分区：是把内存中不用的进程和存到硬盘上的swap分区，当cpu调用时，再从硬盘中调回内存。一般来讲，swap 分区容量应大于物理内存大小，建议是内存的两倍，但不超过 2GB。

建立新的 swap 分区，只需要执行以下几个步骤。

1. 分区：不管是 fdisk 命令还是 parted 命令，都需要先区。

   ```
   fdisk /dev/sdb
   ```

2. 格式化：格式化命令稍有不同，使用 mkswap 命令把分区格式化成 swap 分区。

   ```
   [root@localhost ~]# mkswap /dev/sdb1
   Setting up swapspace version 1, size = 522076 KiB
   no label, UUID=c3351 dc3-f403-419a-9666-c24615e170fb
   ```

3. 使用 swap 分区。

   ```xml
   [root@localhost ~]#free
   total used free shared buffers cached
   Mem: 1030796 130792 900004 0 15292 55420
   -/+ buffers/cache: 60080 970716
   Swap: 2047992 0 2047992
   ```

   free 命令主要是用来查看内存和 swap 分区的使用情况的，其中：

   - total：是指总数；
   - used：是指已经使用的；
   - free：是指空闲的；
   - shared：是指共享的；
   - buffers：是指缓冲内存数；
   - cached：是指缓存内存数，单位是KB；

### RAM和ROM：

RAM：其实就是电脑或手机的运行内存，存放手机软件运行后进行的数据交换等工作。

ROM：其实就是电脑或手机的硬盘，可以存储各种各样的文件，包括视频、照片、音乐、软件等

### 一个卡顿实例（由于swap分区占用比例高导致GC时间长）：

有一个关于服务的某个实例，经常发生服务卡顿。由于服务的并发量是比较高的，所以表现也非常明显。这个服务和我们第 8 课时的高并发服务类似，每多停顿 1 秒钟，几万用户的请求就会感到延迟。

我们统计、类比了此服务其他实例的 CPU、内存、网络、I/O 资源，区别并不是很大，所以一度怀疑是机器硬件的问题。

接下来我们对比了节点的 GC 日志，发现无论是 Minor GC，还是 Major GC，这个节点所花费的时间，都比其他实例长得多。

通过仔细观察，我们发现在 GC 发生的时候，vmstat 的 si、so 飙升的非常严重，这和其他实例有着明显的不同。

使用 free 命令再次确认，发现 SWAP 分区，使用的比例非常高，引起的具体原因是什么呢？

更详细的操作系统内存分布，从 /proc/meminfo 文件中可以看到具体的逻辑内存块大小，有多达 40 项的内存信息，这些信息都可以通过遍历 /proc 目录的一些文件获取。我们注意到 slabtop 命令显示的有一些异常，dentry（目录高速缓冲）占用非常高。

问题最终定位到是由于某个运维工程师执行了一句命令：

```java
find / | grep "x"
```


他是想找一个叫做 x 的文件，看看在哪台服务器上，结果，这些老服务器由于文件太多，扫描后这些文件信息都缓存到了 slab 区上。而服务器开了 swap，操作系统发现物理内存占满后，并没有立即释放 cache，导致每次 GC 都要和硬盘打一次交道。

解决方式就是关闭 SWAP 分区。

**swap 是很多性能场景的万恶之源，建议禁用。**当你的应用真正高并发了，SWAP 绝对能让你体验到它魔鬼性的一面：**进程倒是死不了了，但 GC 时间长的却让人无法忍受。**

### 内存泄漏

![img](https://s0.lgstatic.com/i/image3/M01/66/80/Cgq2xl5GcZ-AP5pOAAAhN4DWbUQ769.jpg)

我们再来聊一下内存溢出和内存泄漏的区别。

**内存溢出是一个结果，而内存泄漏是一个原因。**内存溢出的原因有内存空间不足、配置错误等因素。

不再被使用的对象、没有被回收、没有及时切断与 GC Roots 的联系，这就是内存泄漏。内存泄漏是一些错误的编程方式，或者过多的无用对象创建引起的。

举个例子，有团队使用了 HashMap 做缓存，但是并没有设置超时时间或者 LRU 策略，造成了放入 Map 对象的数据越来越多，而产生了内存泄漏。

再来看一个经常发生的内存泄漏的例子，也是由于 HashMap 产生的。代码如下，由于没有重写 Key 类的 hashCode 和 equals 方法，造成了放入 HashMap 的所有对象都无法被取出来，它们和外界**失联了**。所以下面的代码结果是 null。

```java
//leak example
import java.util.HashMap;
import java.util.Map;

public class HashMapLeakDemo {
    public static class Key {
        String title;

        public Key(String title) {
            this.title = title;
        }
    }
    
    public static void main(String[] args) {
        Map<Key, Integer> map = new HashMap<>();
    
        map.put(new Key("1"), 1);
        map.put(new Key("2"), 2);
        map.put(new Key("3"), 2);
    
        Integer integer = map.get(new Key("2"));
        System.out.println(integer);
    }

}
```


即使提供了 equals 方法和 hashCode 方法，也要非常小心，尽量避免使用自定义的对象作为 Key。仓库中 dog 目录有一个实际的、有问题的例子，你可以尝试排查一下。

再看一个例子，关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally 块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。

另外，对 Java API 的一些不当使用，也会造成内存泄漏。很多同学喜欢使用 String 的 intern 方法，但如果字符串本身是一个非常长的字符串，而且创建之后不再被使用，则会造成内存泄漏。

```java
import java.util.UUID;

public class InternDemo {
    static String getLongStr() {
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < 100000; i++) {
            sb.append(UUID.randomUUID().toString());
        }
        return sb.toString();
    }

    public static void main(String[] args) {
        while (true) {
            getLongStr().intern();
        }
    }

}
```

### MAT介绍：

MAT：排查堆的工具，在启动参数加如下参数，然后导入MAT查看对详情。

```java
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/admin/logs/java.hprof
```

##### MAT界面图：

##### ![](D:\study\学习资料\笔记\JVM\图片\第一讲\Cgq2xl5NRsiAGcDYAAGoA74s9J4470.jpg)

##### MAT的一些参数说明：

- incoming references ：对象的引入（被引用谁）

- outgoing references：对象的引出（引用谁）
- Shallow Heap：浅堆
- Retained Heap：深堆

如下图，C的引入被A、B引用、C引用D、E：

![image-20210204211309891](C:\Users\12031\AppData\Roaming\Typora\typora-user-images\image-20210204211309891.png)

##### 对应的MAT图：

入口选择查询引入还是引出：

![img](https://s0.lgstatic.com/i/image3/M01/68/10/CgpOIF5NRsiAF2UoAACXuKqZ_nE086.jpg)

##### 代码示例：

```java
public class A {
     private C c1 = C.getInstance();
}
public class B {
     private C c2 = C.getInstance();
}
public class C {
     private static C myC = new C();
     public static C getInstance() {
             return myC;
     }
     private D d1 = new D();
     private E e1 = new E();
}
public class D {
}
public class E {
}
public class SimpleExample {
     public static void main (String argsp[]) throws Exception {
            A a = new A();
            B b = new B();
     }
}
```

##### 代码对应的引入和引出效果图：

引入：

![image-20210204212133206](C:\Users\12031\AppData\Roaming\Typora\typora-user-images\image-20210204212133206.png)

引出：

![image-20210204212209095](C:\Users\12031\AppData\Roaming\Typora\typora-user-images\image-20210204212209095.png)

**浅堆**（Shallow Heap）和**深堆**（Retained Heap）介绍：

- 浅堆代表了对象本身的内存占用，包括对象自身的内存占用，以及“为了引用”其他对象所占用的内存。
- 深堆是一个统计结果，会循环计算引用的具体对象所占用的内存。但是深堆和“对象大小”有一点不同，**深堆指的是一个对象被垃圾回收后，能够释放的内存大小，这些被释放的对象集合，叫做保留集**（Retained Set）。

![img](https://s0.lgstatic.com/i/image3/M01/68/10/CgpOIF5NRsmAEeWTAABDIx8RWa4815.png)

如上图所示，A 对象浅堆大小 1 KB，B 对象 2 KB，C 对象 100 KB。A 对象同时引用了 B 对象和 C 对象，但由于 C 对象也被 D 引用，所以 A 对象的深堆大小为 3 KB（1 KB + 2 KB）。

A 对象大小（1 KB + 2 KB + 100 KB）> A 对象深堆 > A 对象浅堆。

